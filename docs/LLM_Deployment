## Table of Content
- [Table of Content](#table-of-content)
- [Data Parallelism](#data-parallelism)
- [Pipeline Parallelism](#pipeline-parallelism)
- [Tensor Parallelism](#tensor-parallelism)
- [Activation Recomputation](#activation-recomputation)
- [Communication optimization](#communication-optimization)

## Data Parallelism
Data Parallelism

## Pipeline Parallelism
- Use micro batch set up will reduce the bubble size, while increase the memory footprint due to activation cache (but why? isn't that the same as the use the entire batch at once?).
- Use pipeDream Flush to schedule the first back-propagation right after the completion of the first forward propagation. Thus, each device's activation memory space is reduced from O(m) to O(p).
- To further reduce the bubble size, one can adopt model chunks scheme. I.e. each device is assigned to multiple pipeline stages. It reduces the bubble time by $v$, but doesn't affect the ideal processing time. Hence the bubble fraction is reduced too. However, you need to communicate twice as frequent as before.

## Tensor Parallelism
- For column wise split, you still need to perform one all-reduce operation for attention and MLP parallelism, leading to two all-reduce for each of the forward and backward propagation **for each layer**. 

## Activation Recomputation
- The current implementation includes the non-reentrant and the reentrant variants, where the former uses the saved variable hooks from the compute graph, while the latter creates a mini-graph independent from the big graph for each time of the recomputation. The time complexity is generally O(sqrt(n)).
- For pipeline parallelism, a reasonable solution is to discard all the intermediate activation cache while only keep the input activation for each of the pipeline stage.

## Communication optimization
- For tensor-pipeline parallelism across nodes, its possible to do the all reduce after the cross-node communication, which reduces the size of the tensors that you need to deliver, as they perform GPU-to-GPU direct communication.