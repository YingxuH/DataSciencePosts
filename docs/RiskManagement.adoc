= Risk Management Analysis

:toc:
:toc-title:
:toc-placement: preamble
:sectnums:
:imagesDir: ../images
:stylesDir: stylesheets
:xrefstyle: full
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:warning-caption: :warning:
endif::[]

By: `He Yingxu`

== Introduction

This is a course project in BT4211 Risk Analysis, where we

* Compared accuracy and MCC of logistic regression using the full features and lasso
regression keeping only half of the features on a loan-default prediction task.
These two models show a similar accuracy and <<mcc, MCC>>,
which implies the selected variables by lasso regression is equivalent to the full features.

* we compared the performance of logistic regression, Lasso regression, decision tree,
and XGBoost, and identified the XGBoost as the most
effective model with an accuracy of 0.90 and a <<mcc, MCC>> of 0.81.

* Tunned the hyper-parameters of XGBoost through both manual and automatic tunning (e.g. bayesian
optimisation and genetic algorithm) to achieve an accuracy of 0.924.
Experimented essemble methods such as stacking, majority voting, etc.

Find the code https://github.com/YingxuH/DataSciencePosts/blob/master/src/Assignment2.ipynb[here].

=== Data Set
A data set containing loans data issued to customers. It includes the current loan status
(Current, Late, Fully Paid, etc.) and a large set of attributes for each customer.

=== Goal
Predict if a load will default using all the information given.

== Logistic Regression

The <<link-function, link function>> for Logit regression is log(<<odds, odds>>) =
alpha + beta*x + .... +
p(x) = e^(alpha + beta*X) / 1 + e^(alpha + beta*X)

=== Adjusted R-square

The drawback of R^2 is that if new predictors are added to the model, R^2 only increases
or remains constant but it never decreases. +

The Adjusted R-Square is the modified form of R-Square that has been adjusted
for the number of predictors in the model. It incorporates modelâ€™s degree of freedom.
The adjusted R-Square only increases if the new term improves the model accuracy. +

image::adj-r2.png[]

=== Loss Function

=== Log-likelihood

The probability of one data point is

image::logit-probability.png[]

Thus, the probability of all the data is

image::logit-all-data-prob.png[]

Taking log of this function, the log-likehood equation is:

image::log-likelihood.png[]

=== Solver

=== lasso regression

apply l1 regularization on the regression. The goal of the training is turned to

image::lasso-loss.png[]

Even at small alphas, the coefficients are reducing towards 0. Lasso selects only
some features while reduces the coefficients of others to zero.

=== Ridge regression

The regularization term is changed to the square of weight vector. It helps to reduce the model
complexity and prevent over fitting, but cannot select features.

== Tree Classification

== XGBoost


== Keywords
[[odds]] Odds::
p(x)/1-p(x)

[[link-function]] Link Function::
The function specifies how u = E(Y) relates to explanatory variable in the
linear predictor.

[[mcc]] MCC::
https://en.wikipedia.org/wiki/Matthews_correlation_coefficient[Matthews Correlation Coefficient]:
While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number,
the Matthews correlation coefficient is generally regarded as being one of the best such measures.
Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes.
For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.
