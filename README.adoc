= DataSciencePosts

:toc:
:toc-title:
:toc-placement: preamble
:sectnums:
:imagesDir: images
:stylesDir: stylesheets
:xrefstyle: full
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:warning-caption: :warning:
endif::[]

= Machine Learning

=== Bagging and boosting

= Deep Learning

== activation function

softmax:

== Loss function

=== cross-entropy

Entroy is the the number of bits required to transmit a randomly selected event
from a probabilty distribution. A skewd distribution has a low entropy whereas a
distribution where events have equal probabilty has a larger entropy.

Entropy for an event: H(x) = -log(p(x))

image::Plot-of-Probability-vs-Information.png[]

Entropy for a random variable H(x) = -sum(each k in K, p(k)*log(p(k)))

For binary classification: `H(P, Q) = – (P(class0) * log(Q(class0)) + P(class1) * log(Q(class1)))`

https://machinelearningmastery.com/cross-entropy-for-machine-learning/[Multi-class classification]:
The number of nodes in the last layer should be equal to the number of classes.

. Calculate the entropy between predicted prob and ground truth for each class
. Average the cross entropy

Details link:\\cross_entropy.py[here].
[source.python]
----
include::cross_entropy.py[lines:10..16]
----

=== log likelihood

=== Newton's Method

Similar to Gradient Descent, we firstly take the partial derivative of J(θ) that is the slope of J(θ),
and note it as f(θ). Instead of decreasing θ by a certain chosen learning rate `α` multiplied with f(θ) ,
Newton’s Method gets an updated θ at the `point of intersection of the tangent line of f(θ) at previous θ and x axis`.
After amount of iterations, Newton’s Method will converge at f(θ) = 0.

== Overfitting:

=== Batch normalization

==== What

Backpropagation assumes the othher layers do not change while it updates the parameter
for the layers. Because all layers are changed during an update, the update procedure
is forever chasing a moving target.

For example, the weights of a layer are updated given an expectation that the prior
 layer outputs values with a given distribution. This distribution is likely changed after the weights of the prior layer are updated.

* Forward Learning
* Calculate gradient based on the previous batch input
* Input from the next batch may have a different distribution where the gradient
does not apply.

==== How:

standardizing the activation of the prior layers. Thus, the assumption about the  spread
and distribution made by the next layer will not change, at least not dramatically.

[[standardization]] Standardization::
rescaling the data to have a mean of zero and std of one.

==== Effect:

* Speed up the training process.
* reducing generalization error, like regularization.
* Makes the training more stable, thus allows larger learning rate.
* works well with optimization performance, especially for cnn and networks with
sigmoidal nonlinearities.


=== Complexity


The number of hidden nodes will affect the training outcome.
How to decide the number of units?

== Convelutional Nerual Network

== RNN

=== Number of units:

https://keras.io/layers/embeddings/[Keras Embedding layer]
Embedding(input_dim, output_dim, input_length)

[[input_dim]] input_dim::
size of the vocabulary, maximum integer index + 1

[[output_dim]] output_dim::
Dimension of the dense word embedding.


Cannot use batch normalization in RNN as it does not consider the recurrent part
of the network.

== Related Work
